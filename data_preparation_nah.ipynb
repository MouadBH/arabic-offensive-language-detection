{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_for_classification(filename, header=True,delim=\"\\t\"):\n",
    "    ids = []\n",
    "    data=[]\n",
    "    with open(filename, 'rU') as f:\n",
    "        if header:\n",
    "            next(f)\n",
    "        for line in f:\n",
    "            row =line.split(delim)\n",
    "            # row should have 2 entries -- ID \\t Text_Content\n",
    "            row_id = row[0]\n",
    "            text = row[1]\n",
    "            text = clean_content(text)\n",
    "            if isinstance(text, str):\n",
    "                data.append(text)\n",
    "                ids.append(row_id)\n",
    "    return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_a_shower(line):\n",
    "    if (isinstance(line, float)):\n",
    "        return None\n",
    "    line.replace('\\n', ' ')\n",
    "    line = remove_emails(line)\n",
    "    line = remove_urls(line)\n",
    "    nline = [w if '@' not in w else 'USERIDX' for w in line.split()]\n",
    "    line = ' '.join(nline)\n",
    "    line = line.replace('RT', '').replace('<LF>', '').replace('<br />','').replace('&quot;', '').replace('<url>', '')\n",
    "\n",
    "\n",
    "    # add spaces between punc,\n",
    "    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations_list}))\n",
    "\n",
    "    # then remove punc,\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    line = line.translate(translator)\n",
    "\n",
    "    line=remove_diacritics(normalize_arabic(line))\n",
    "\n",
    "    line = remove_stopwords(line)\n",
    "\n",
    "    #replace number\n",
    "    nline = [word if not hasDigits(word) else '<NUM>' for word in line.split()]\n",
    "    line = ' '.join(nline)\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDigits(s):\n",
    "    return any( 48 <= ord(char) <= 57  or 1632 <= ord(char) <= 1641 for char in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls (text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_emails(text):\n",
    "    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI\n",
    "\n",
    "# add space near your emoji\n",
    "def add_space_with_emojis(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeConsecutiveSameNum(v):\n",
    "    st = []\n",
    "    lines=[]\n",
    "\n",
    "    # Start traversing the sequence\n",
    "    for i in range(len(v)):\n",
    "\n",
    "        # Push the current string if the stack\n",
    "        # is empty\n",
    "        if (len(st) == 0):\n",
    "            st.append(v[i])\n",
    "            lines.append(v[i])\n",
    "        else:\n",
    "            Str = st[-1]\n",
    "\n",
    "            # compare the current string with stack top\n",
    "            # if equal, pop the top\n",
    "            if (Str == v[i] and Str == '<NUM>'):\n",
    "                st.pop()\n",
    "\n",
    "                # Otherwise push the current string\n",
    "            else:\n",
    "                lines.append(v[i])\n",
    "                st.pop()\n",
    "                # st.append(v[i])\n",
    "\n",
    "                # Return stack size\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Normalization\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_sentence = [w for w in text.split() if not w in arb_stopwords]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean/Normalize Arabic Text Based on aravec\n",
    "def clean_content_aravec(line):\n",
    "\n",
    "    if (isinstance(line, float)):\n",
    "        return None\n",
    "    line.replace('\\n', ' ')\n",
    "    line = remove_emails(line)\n",
    "    line = remove_urls(line)\n",
    "    line = line.replace('@User', '').replace('RT', '').replace('<LF>', '')\n",
    "\n",
    "    # Check if # or @ is there with word\n",
    "\n",
    "    # add spaces between punc,\n",
    "    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations_list}))\n",
    "\n",
    "    # then remove punc,\n",
    "    # line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    line = line.translate(translator)\n",
    "\n",
    "    search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\\\", '\\n', '\\t',\n",
    "              '&quot;', '?', '؟', '!']\n",
    "    replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", ' ', ' ', ' ', ' ? ', ' ؟ ',\n",
    "               ' ! ']\n",
    "\n",
    "    # remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    line = re.sub(p_tashkeel, \"\", line)\n",
    "\n",
    "    # remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    line = re.sub(p_longation, subst, line)\n",
    "\n",
    "    line = line.replace('وو', 'و')\n",
    "    line = line.replace('يي', 'ي')\n",
    "    line = line.replace('اا', 'ا')\n",
    "\n",
    "    for i in range(0, len(search)):\n",
    "        line = line.replace(search[i], replace[i])\n",
    "\n",
    "    # trim\n",
    "    line = line.strip()\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def map_labels_off(lab):\n",
    "    label_maps = {\"Non-Offensive\": \"NOT_OFF\", \"Offensive\": \"OFF\"}\n",
    "    if lab in label_maps:\n",
    "        return label_maps[lab]\n",
    "    else:\n",
    "        return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'عجبكش الفيديو فان جاعر ديال منال'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_a_shower('ما عجبكش الفيديو = فان جاعر ديال منال\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
